{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_yV-sF-XP7zt"
      },
      "source": [
        "# AI project team 16\n",
        "Implementing the Vision Transformer (ViT) model for American Sign Language (ASL) image classification.\n",
        "\n",
        "Dataset: Sign Language MNIST.\n",
        "\n",
        "The code are partially based on the [ViT model](https://keras.io/examples/vision/image_classification_with_vision_transformer/) on keras.\n",
        "[ViT](https://arxiv.org/abs/2010.11929)\n",
        "are proposed by Alexey Dosovitskiy et al. for image classification, which applies the Transformer architecture with self-attention to sequences of\n",
        "image patches, without using convolution layers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BF4CNEXQP7zu"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4kbsoSO0P7zu"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "!pip install -U tensorflow-addons\n",
        "import tensorflow_addons as tfa"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qwYGfwGOP7zu"
      },
      "source": [
        "## Data loading and pre-processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ScQtkQREP7zv"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "import os\n",
        "import cv2\n",
        "\n",
        "basePath = ''\n",
        "train = pd.read_csv(basePath + \"sign_mnist_train.csv\")\n",
        "test = pd.read_csv(basePath + \"sign_mnist_test.csv\")\n",
        "\n",
        "# generate pictures stored path\n",
        "if not os.path.exists(basePath + \"train_pic\"):\n",
        "    os.mkdir(basePath + \"train_pic\")\n",
        "if not os.path.exists(basePath + \"test_pic\"):\n",
        "    os.mkdir(basePath + \"test_pic\")\n",
        "\n",
        "train_pic_path = basePath + \"train_pic/\"\n",
        "test_pic_path = basePath + \"test_pic/\"\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# generate pictures\n",
        "for i, row in enumerate(train.to_numpy()):\n",
        "    label = row[0]\n",
        "    data = row[1:]\n",
        "    data = data.reshape((28, 28))\n",
        "    if not os.path.exists(f'{train_pic_path}{label}'):\n",
        "        os.mkdir(f'{train_pic_path}{label}')\n",
        "    cv2.imwrite(f'{train_pic_path}{label}/{i}.jpeg', data)\n",
        "\n",
        "for i, row in enumerate(test.to_numpy()):\n",
        "    label = row[0]\n",
        "    data = row[1:]\n",
        "    data = data.reshape((28, 28))\n",
        "    if not os.path.exists(f'{test_pic_path}{label}'):\n",
        "        os.mkdir(f'{test_pic_path}{label}')\n",
        "    cv2.imwrite(f'{test_pic_path}{label}/{i}.jpeg', data)\n",
        "\n",
        "y_train = train['label'].values\n",
        "# x_train = train.drop(columns=['label']).to_numpy().reshape((train.shape[0], 28, 28, 1)).astype('float64') / 255.0\n",
        "x_train = train.drop(columns=['label']).to_numpy().reshape((train.shape[0], 28, 28, 1)).astype('float64')\n",
        "\n",
        "\n",
        "y_test = test['label'].values\n",
        "# x_test = test.drop(columns=['label']).to_numpy().reshape((test.shape[0], 28, 28, 1)).astype('float64') / 255.0\n",
        "x_test = test.drop(columns=['label']).to_numpy().reshape((test.shape[0], 28, 28, 1)).astype('float64')\n",
        "\n",
        "# y_train = to_categorical(y_train)\n",
        "# y_test = to_categorical(y_test)\n",
        "\n",
        "print(x_train.shape)\n",
        "print(x_test.shape)\n",
        "print(y_train.shape)\n",
        "print(y_test.shape)"
      ],
      "metadata": {
        "id": "aXacWxnyxnJO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JSjRDjM-P7zv"
      },
      "source": [
        "## Hyperparameters setting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mN9nGQFbP7zv"
      },
      "outputs": [],
      "source": [
        "# pad1=np.full((27455,28, 28,2), 0)\n",
        "# pad2=np.full((7172,28, 28,2), 0)\n",
        "# x_train=np.concatenate((x_train, pad1),axis=3)\n",
        "# x_test = np.concatenate((x_test, pad2), axis=3)\n",
        "\n",
        "x_train=np.concatenate((x_train, x_train, x_train),axis=3)\n",
        "x_test = np.concatenate((x_test, x_test, x_test), axis=3)\n",
        "print(x_train.shape)\n",
        "print(x_test.shape)\n",
        "x_train = np.pad(x_train, ((0, 0), (2,2), (2,2), (0,0)), 'constant', constant_values=0.9)\n",
        "x_test = np.pad(x_test, ((0, 0), (2,2), (2,2), (0,0)), 'constant', constant_values=0.9)\n",
        "print(x_train.shape)\n",
        "print(x_test.shape)\n",
        "\n",
        "input_shape = (32, 32, 3)\n",
        "\n",
        "num_classes = 25\n",
        "learning_rate = 0.001\n",
        "weight_decay = 0.0001\n",
        "batch_size = 256\n",
        "num_epochs = 100\n",
        "image_size = 72  # We'll resize input images to this size\n",
        "patch_size = 6  # Size of the patches to be extract from the input images\n",
        "num_patches = (image_size // patch_size) ** 2\n",
        "projection_dim = 64\n",
        "num_heads = 4\n",
        "transformer_units = [\n",
        "    projection_dim * 2,\n",
        "    projection_dim,\n",
        "]  # Size of the transformer layers\n",
        "transformer_layers = 8\n",
        "mlp_head_units = [2048, 1024]  # Size of the dense layers of the final classifier\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B1V9c2thP7zw"
      },
      "source": [
        "## Data augmentation\n",
        "\n",
        "Apply data augmentation on images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W9OGijv2P7zw"
      },
      "outputs": [],
      "source": [
        "data_augmentation = keras.Sequential(\n",
        "    [\n",
        "        layers.Normalization(),\n",
        "        layers.Resizing(image_size, image_size),\n",
        "        layers.RandomFlip(\"horizontal\"),\n",
        "        layers.RandomRotation(factor=0.02),\n",
        "        layers.RandomZoom(\n",
        "            height_factor=0.2, width_factor=0.2\n",
        "        ),\n",
        "    ],\n",
        "    name=\"data_augmentation\",\n",
        ")\n",
        "# Compute the mean and the variance of the training data for normalization.\n",
        "data_augmentation.layers[0].adapt(x_train)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kdIZ2HuFP7zw"
      },
      "source": [
        "## MLP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6qF6EaeNP7zw"
      },
      "outputs": [],
      "source": [
        "\n",
        "def mlp(x, hidden_units, dropout_rate):\n",
        "    for units in hidden_units:\n",
        "        x = layers.Dense(units, activation=tf.nn.gelu)(x)\n",
        "        x = layers.Dropout(dropout_rate)(x)\n",
        "    return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rx_of4XlP7zx"
      },
      "source": [
        "## Patch creation layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fD0W5y55P7zx"
      },
      "outputs": [],
      "source": [
        "\n",
        "class Patches(layers.Layer):\n",
        "    def __init__(self, patch_size):\n",
        "        super(Patches, self).__init__()\n",
        "        self.patch_size = patch_size\n",
        "\n",
        "    def call(self, images):\n",
        "        batch_size = tf.shape(images)[0]\n",
        "        patches = tf.image.extract_patches(\n",
        "            images=images,\n",
        "            sizes=[1, self.patch_size, self.patch_size, 1],\n",
        "            strides=[1, self.patch_size, self.patch_size, 1],\n",
        "            rates=[1, 1, 1, 1],\n",
        "            padding=\"VALID\",\n",
        "        )\n",
        "        patch_dims = patches.shape[-1]\n",
        "        patches = tf.reshape(patches, [batch_size, -1, patch_dims])\n",
        "        return patches\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lkwrtlpDP7zx"
      },
      "source": [
        "Let's display patches for a sample image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zLZuyGPHP7zx"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(4, 4))\n",
        "image = x_train[np.random.choice(range(x_train.shape[0]))]\n",
        "plt.imshow(image.astype(\"uint8\"))\n",
        "plt.axis(\"off\")\n",
        "\n",
        "resized_image = tf.image.resize(\n",
        "    tf.convert_to_tensor([image]), size=(image_size, image_size)\n",
        ")\n",
        "patches = Patches(patch_size)(resized_image)\n",
        "print(f\"Image size: {image_size} X {image_size}\")\n",
        "print(f\"Patch size: {patch_size} X {patch_size}\")\n",
        "print(f\"Patches per image: {patches.shape[1]}\")\n",
        "print(f\"Elements per patch: {patches.shape[-1]}\")\n",
        "\n",
        "n = int(np.sqrt(patches.shape[1]))\n",
        "plt.figure(figsize=(4, 4))\n",
        "for i, patch in enumerate(patches[0]):\n",
        "    ax = plt.subplot(n, n, i + 1)\n",
        "    patch_img = tf.reshape(patch, (patch_size, patch_size, 3))\n",
        "    plt.imshow(patch_img.numpy().astype(\"uint8\"))\n",
        "    plt.axis(\"off\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vz07YpMUP7zx"
      },
      "source": [
        "## Patch encoding layer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ao0X1oFHP7zx"
      },
      "outputs": [],
      "source": [
        "\n",
        "class PatchEncoder(layers.Layer):\n",
        "    def __init__(self, num_patches, projection_dim):\n",
        "        super(PatchEncoder, self).__init__()\n",
        "        self.num_patches = num_patches\n",
        "        self.projection = layers.Dense(units=projection_dim)\n",
        "        self.position_embedding = layers.Embedding(\n",
        "            input_dim=num_patches, output_dim=projection_dim\n",
        "        )\n",
        "\n",
        "    def call(self, patch):\n",
        "        positions = tf.range(start=0, limit=self.num_patches, delta=1)\n",
        "        encoded = self.projection(patch) + self.position_embedding(positions)\n",
        "        return encoded\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XT9IEaBFP7zy"
      },
      "source": [
        "## Construct the model\n",
        "\n",
        "The ViT model that consists of multiple Transformer blocks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5BBX6167P7zy"
      },
      "outputs": [],
      "source": [
        "\n",
        "def create_vit_classifier():\n",
        "    inputs = layers.Input(shape=input_shape)\n",
        "    # Augment data.\n",
        "    augmented = data_augmentation(inputs)\n",
        "    # Create patches.\n",
        "    patches = Patches(patch_size)(augmented)\n",
        "    # Encode patches.\n",
        "    encoded_patches = PatchEncoder(num_patches, projection_dim)(patches)\n",
        "\n",
        "    # Create multiple layers of the Transformer block.\n",
        "    for _ in range(transformer_layers):\n",
        "        # Layer normalization 1.\n",
        "        x1 = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
        "        # Create a multi-head attention layer.\n",
        "        attention_output = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=projection_dim, dropout=0.1\n",
        "        )(x1, x1)\n",
        "        # Skip connection 1.\n",
        "        x2 = layers.Add()([attention_output, encoded_patches])\n",
        "        # Layer normalization 2.\n",
        "        x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n",
        "        # MLP.\n",
        "        x3 = mlp(x3, hidden_units=transformer_units, dropout_rate=0.1)\n",
        "        # Skip connection 2.\n",
        "        encoded_patches = layers.Add()([x3, x2])\n",
        "\n",
        "    # Create a [batch_size, projection_dim] tensor.\n",
        "    representation = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
        "    representation = layers.Flatten()(representation)\n",
        "    representation = layers.Dropout(0.5)(representation)\n",
        "    # Add MLP.\n",
        "    features = mlp(representation, hidden_units=mlp_head_units, dropout_rate=0.5)\n",
        "    # Classify outputs.\n",
        "    logits = layers.Dense(num_classes)(features)\n",
        "    # Create the Keras model.\n",
        "    model = keras.Model(inputs=inputs, outputs=logits)\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "84JH1bO1P7zy"
      },
      "source": [
        "## Mode training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oju0Pq82P7zz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3b025d28-a8bb-41e2-9a2f-e6ce08e2ca77"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "97/97 [==============================] - 1508s 15s/step - loss: 2.9100 - accuracy: 0.1825 - top-5-accuracy: 0.5212 - val_loss: 1.7488 - val_accuracy: 0.4701 - val_top-5-accuracy: 0.8664\n",
            "Epoch 2/100\n",
            "97/97 [==============================] - 1489s 15s/step - loss: 1.6706 - accuracy: 0.4623 - top-5-accuracy: 0.8544 - val_loss: 0.7682 - val_accuracy: 0.7720 - val_top-5-accuracy: 0.9822\n",
            "Epoch 3/100\n",
            "97/97 [==============================] - 1489s 15s/step - loss: 1.0722 - accuracy: 0.6409 - top-5-accuracy: 0.9515 - val_loss: 0.3627 - val_accuracy: 0.8973 - val_top-5-accuracy: 0.9978\n",
            "Epoch 4/100\n",
            "97/97 [==============================] - 1472s 15s/step - loss: 0.7399 - accuracy: 0.7462 - top-5-accuracy: 0.9812 - val_loss: 0.2220 - val_accuracy: 0.9436 - val_top-5-accuracy: 0.9993\n",
            "Epoch 5/100\n",
            "97/97 [==============================] - 1494s 15s/step - loss: 0.5689 - accuracy: 0.8059 - top-5-accuracy: 0.9911 - val_loss: 0.1193 - val_accuracy: 0.9752 - val_top-5-accuracy: 0.9996\n",
            "Epoch 6/100\n",
            "97/97 [==============================] - 1482s 15s/step - loss: 0.4161 - accuracy: 0.8546 - top-5-accuracy: 0.9954 - val_loss: 0.0809 - val_accuracy: 0.9782 - val_top-5-accuracy: 1.0000\n",
            "Epoch 7/100\n",
            "97/97 [==============================] - 1477s 15s/step - loss: 0.3146 - accuracy: 0.8896 - top-5-accuracy: 0.9970 - val_loss: 0.0479 - val_accuracy: 0.9913 - val_top-5-accuracy: 1.0000\n",
            "Epoch 8/100\n",
            "97/97 [==============================] - 1493s 15s/step - loss: 0.2707 - accuracy: 0.9079 - top-5-accuracy: 0.9983 - val_loss: 0.0261 - val_accuracy: 0.9942 - val_top-5-accuracy: 1.0000\n",
            "Epoch 9/100\n",
            "25/97 [======>.......................] - ETA: 17:48 - loss: 0.2499 - accuracy: 0.9144 - top-5-accuracy: 0.9992"
          ]
        }
      ],
      "source": [
        "\n",
        "def run_experiment(model):\n",
        "    optimizer = tfa.optimizers.AdamW(\n",
        "        learning_rate=learning_rate, weight_decay=weight_decay\n",
        "    )\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=optimizer,\n",
        "        loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "        metrics=[\n",
        "            keras.metrics.SparseCategoricalAccuracy(name=\"accuracy\"),\n",
        "            keras.metrics.SparseTopKCategoricalAccuracy(5, name=\"top-5-accuracy\"),\n",
        "        ],\n",
        "    )\n",
        "\n",
        "    checkpoint_filepath = \"/tmp/checkpoint\"\n",
        "    checkpoint_callback = keras.callbacks.ModelCheckpoint(\n",
        "        checkpoint_filepath,\n",
        "        monitor=\"val_accuracy\",\n",
        "        save_best_only=True,\n",
        "        save_weights_only=True,\n",
        "    )\n",
        "\n",
        "    history = model.fit(\n",
        "        x=x_train,\n",
        "        y=y_train,\n",
        "        batch_size=batch_size,\n",
        "        epochs=num_epochs,\n",
        "        validation_split=0.1,\n",
        "        callbacks=[checkpoint_callback],\n",
        "    )\n",
        "\n",
        "    model.load_weights(checkpoint_filepath)\n",
        "    _, accuracy, top_5_accuracy = model.evaluate(x_test, y_test)\n",
        "    print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n",
        "    print(f\"Test top 5 accuracy: {round(top_5_accuracy * 100, 2)}%\")\n",
        "\n",
        "    return history\n",
        "\n",
        "\n",
        "vit_classifier = create_vit_classifier()\n",
        "history = run_experiment(vit_classifier)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YPGJiFbSP7zz"
      },
      "source": [
        "## Evaluation results\n",
        "After 8 epochs, the ViT model achieves around 99.2% accuracy and 100% top-5 accuracy on the ASL test data. \n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_model_history(history):\n",
        "  plt.plot(history.history['accuracy'])\n",
        "  plt.plot(history.history['val_accuracy'])\n",
        "  plt.title('Accuracy')\n",
        "  plt.ylabel('Accuracy')\n",
        "  plt.xlabel('epoch')\n",
        "  plt.legend(['train', 'validation'], loc='upper left')\n",
        "  plt.show()\n",
        "\n",
        "  plt.plot(history.history['top-5-accuracy'])\n",
        "  plt.plot(history.history['val_top-5-accuracy'])\n",
        "  plt.title('Top 5 Accuracy')\n",
        "  plt.ylabel('Accuracy')\n",
        "  plt.xlabel('epoch')\n",
        "  plt.legend(['train', 'validation'], loc='upper left')\n",
        "  plt.show()\n",
        "  # loss\n",
        "  plt.plot(history.history['loss'])\n",
        "  plt.plot(history.history['val_loss'])\n",
        "  plt.title('Loss')\n",
        "  plt.ylabel('Loss')\n",
        "  plt.xlabel('epoch')\n",
        "  plt.legend(['train', 'validation'], loc='upper left')\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "plot_model_history(history)"
      ],
      "metadata": {
        "id": "uqBQvUgpsh-b"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "ASL_transformer",
      "provenance": [],
      "toc_visible": true
    },
    "environment": {
      "name": "tf2-gpu.2-4.m61",
      "type": "gcloud",
      "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-4:m61"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}